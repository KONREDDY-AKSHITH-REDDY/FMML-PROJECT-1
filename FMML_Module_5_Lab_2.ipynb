{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMsHEPdekL7CyAwt37eGufm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KONREDDY-AKSHITH-REDDY/FMML-PROJECT-1/blob/main/FMML_Module_5_Lab_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **QUESTION 1)Calculate the Entropy of the above collection of 10 datapoints.**\n",
        "**ANSWER )**\n",
        "\n",
        "To calculate the entropy of a collection of datapoints, you need to consider the distribution of classes in the dataset. Entropy is a measure of impurity or disorder in a dataset. H(X)=−∑P(xi)⋅log2(P(xi)) Where: n is the number of classes, P(xi) is the probability of class i in the dataset.\n",
        "\n",
        "Let's calculate the entropy for the given dataset:\n",
        "\n",
        "Class distribution:\n",
        "\n",
        "Class 0 (setosa): 3 instances\n",
        "Class 1 (versicolor): 4 instances\n",
        "Class 2 (virginica): 3 instances\n",
        "Total instances: 10\n",
        "\n",
        "Now, calculate the probability of each class:\n",
        "\n",
        "P(0)=3/10\n",
        "\n",
        "P(1)=4/10\n",
        "\n",
        "P(2)=3/10\n",
        "\n",
        "Now, plug these probabilities into the entropy formula:\n",
        "\n",
        "H(X) = -(3/10.log(3/10) + 4/10.log(4/10) + 3/10.log(3/10))\n",
        "\n",
        "Therefore, the entropy of the given collection of 10 datapoints is approximately 0.529."
      ],
      "metadata": {
        "id": "8v8pH4SN01OJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **QUESTON 2)Suggest a decision node (if, else) statement which divides the group into two groups. Also compute the Information Gain in that division step. Compare this with other decision clauses that you can make and intuitively comment on which is better for classification and observe if this has any correlation with the numerical value of Information Gain.**\n",
        "**ANSWER )**\n",
        "\n",
        "To create a decision node for dividing the group into two subgroups, we need to choose a feature and a threshold to split the data. One possible decision node could be based on the petal width feature. Let's consider the following decision node:"
      ],
      "metadata": {
        "id": "-L9ZisrD0_hD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if petal_width <= 1.5:\n",
        "    predicted_class = 'setosa'\n",
        "else:\n",
        "    predicted_class = 'versicolor or virginica'"
      ],
      "metadata": {
        "id": "JS7aizce1NfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's compute the Information Gain for this decision:\n",
        "\n",
        "1)Calculate the entropy before the split (initial entropy):\n",
        "\n",
        "H(initial) = -(3/10.log(3/10) + 4/10.log(4/10) + 3/10.log(3/10))\n",
        "\n",
        "2)Calculate the weighted average of entropy after the split:\n",
        "\n",
        "H(split)= 3/10.(setosa)+ 7/10.H(versicolor or virginica)\n",
        "\n",
        "Here,H(setosa)=0(since all instances in Group 1 are of class setosa), and H(versicolor or virginica) needs to be calculated similarly to the initial entropy.\n",
        "\n",
        "3)Information Gain:\n",
        "\n",
        "Information Gain = H(initial) - H(split)\n",
        "\n",
        "Compare this Information Gain value with other potential decision nodes. The decision node with the highest Information Gain is generally preferred because it reduces the overall entropy the most."
      ],
      "metadata": {
        "id": "ElWWlaNp1WWA"
      }
    }
  ]
}